{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994598c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not find GPU, using the CPU\n",
      "(400, 4)\n",
      "3\n",
      "5\n",
      "(6,)\n",
      "0\n",
      "T=5 Epoch: 1/40 - Loss: 0.8688 b_loss=0.8688 - T: 0.12s/epoch ,tempo_previto=3.88 min0.05 nl5 QC0\n",
      "1\n",
      "T=5 Epoch: 2/40 - Loss: 0.8409 b_loss=0.8409 - T: 0.05s/epoch ,tempo_previto=1.56 min0.05 nl5 QC0\n",
      "2\n",
      "T=5 Epoch: 3/40 - Loss: 0.8218 b_loss=0.8218 - T: 0.05s/epoch ,tempo_previto=1.66 min0.05 nl5 QC0\n",
      "3\n",
      "T=5 Epoch: 4/40 - Loss: 0.7959 b_loss=0.7959 - T: 0.05s/epoch ,tempo_previto=1.56 min0.05 nl5 QC0\n",
      "4\n",
      "T=5 Epoch: 5/40 - Loss: 0.7660 b_loss=0.7660 - T: 0.05s/epoch ,tempo_previto=1.56 min0.05 nl5 QC0\n",
      "5\n",
      "T=5 Epoch: 6/40 - Loss: 0.7475 b_loss=0.7475 - T: 0.05s/epoch ,tempo_previto=1.59 min0.05 nl5 QC0\n",
      "6\n",
      "T=5 Epoch: 7/40 - Loss: 0.7297 b_loss=0.7297 - T: 0.05s/epoch ,tempo_previto=1.56 min0.05 nl5 QC0\n",
      "7\n",
      "T=5 Epoch: 8/40 - Loss: 0.7022 b_loss=0.7022 - T: 0.05s/epoch ,tempo_previto=1.65 min0.05 nl5 QC0\n",
      "8\n",
      "T=5 Epoch: 9/40 - Loss: 0.6939 b_loss=0.6939 - T: 0.05s/epoch ,tempo_previto=1.65 min0.05 nl5 QC0\n",
      "9\n",
      "T=5 Epoch: 10/40 - Loss: 0.6816 b_loss=0.6816 - T: 0.05s/epoch ,tempo_previto=1.59 min0.05 nl5 QC0\n",
      "10\n",
      "T=5 Epoch: 11/40 - Loss: 0.6713 b_loss=0.6713 - T: 0.05s/epoch ,tempo_previto=1.55 min0.05 nl5 QC0\n",
      "11\n",
      "T=5 Epoch: 12/40 - Loss: 0.6578 b_loss=0.6578 - T: 0.05s/epoch ,tempo_previto=1.59 min0.05 nl5 QC0\n",
      "12\n",
      "T=5 Epoch: 13/40 - Loss: 0.6501 b_loss=0.6501 - T: 0.05s/epoch ,tempo_previto=1.58 min0.05 nl5 QC0\n",
      "13\n",
      "T=5 Epoch: 14/40 - Loss: 0.6463 b_loss=0.6463 - T: 0.05s/epoch ,tempo_previto=1.58 min0.05 nl5 QC0\n",
      "14\n",
      "T=5 Epoch: 15/40 - Loss: 0.6350 b_loss=0.6350 - T: 0.05s/epoch ,tempo_previto=1.58 min0.05 nl5 QC0\n",
      "15\n",
      "T=5 Epoch: 16/40 - Loss: 0.6351 b_loss=0.6350 - T: 0.05s/epoch ,tempo_previto=1.55 min0.05 nl5 QC0\n",
      "16\n",
      "T=5 Epoch: 17/40 - Loss: 0.6168 b_loss=0.6168 - T: 0.05s/epoch ,tempo_previto=1.58 min0.05 nl5 QC0\n",
      "17\n",
      "T=5 Epoch: 18/40 - Loss: 0.6183 b_loss=0.6168 - T: 0.05s/epoch ,tempo_previto=1.61 min0.05 nl5 QC0\n",
      "18\n",
      "T=5 Epoch: 19/40 - Loss: 0.6088 b_loss=0.6088 - T: 0.05s/epoch ,tempo_previto=1.58 min0.05 nl5 QC0\n",
      "19\n",
      "T=5 Epoch: 20/40 - Loss: 0.6069 b_loss=0.6069 - T: 0.05s/epoch ,tempo_previto=1.58 min0.05 nl5 QC0\n",
      "20\n",
      "T=5 Epoch: 21/40 - Loss: 0.5998 b_loss=0.5998 - T: 0.05s/epoch ,tempo_previto=1.61 min0.05 nl5 QC0\n",
      "21\n",
      "T=5 Epoch: 22/40 - Loss: 0.6029 b_loss=0.5998 - T: 0.05s/epoch ,tempo_previto=1.61 min0.05 nl5 QC0\n",
      "22\n",
      "T=5 Epoch: 23/40 - Loss: 0.5949 b_loss=0.5949 - T: 0.05s/epoch ,tempo_previto=1.55 min0.05 nl5 QC0\n",
      "23\n",
      "T=5 Epoch: 24/40 - Loss: 0.5875 b_loss=0.5875 - T: 0.05s/epoch ,tempo_previto=1.64 min0.05 nl5 QC0\n",
      "24\n",
      "T=5 Epoch: 25/40 - Loss: 0.5730 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "25\n",
      "T=5 Epoch: 26/40 - Loss: 0.5852 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.54 min0.05 nl5 QC0\n",
      "26\n",
      "T=5 Epoch: 27/40 - Loss: 0.5891 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "27\n",
      "T=5 Epoch: 28/40 - Loss: 0.5730 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "28\n",
      "T=5 Epoch: 29/40 - Loss: 0.5972 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.54 min0.05 nl5 QC0\n",
      "29\n",
      "T=5 Epoch: 30/40 - Loss: 0.5801 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.54 min0.05 nl5 QC0\n",
      "30\n",
      "T=5 Epoch: 31/40 - Loss: 0.5778 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "31\n",
      "T=5 Epoch: 32/40 - Loss: 0.5787 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "32\n",
      "T=5 Epoch: 33/40 - Loss: 0.5856 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.54 min0.05 nl5 QC0\n",
      "33\n",
      "T=5 Epoch: 34/40 - Loss: 0.5825 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.50 min0.05 nl5 QC0\n",
      "34\n",
      "T=5 Epoch: 35/40 - Loss: 0.5786 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.50 min0.05 nl5 QC0\n",
      "35\n",
      "T=5 Epoch: 36/40 - Loss: 0.5760 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "36\n",
      "T=5 Epoch: 37/40 - Loss: 0.5753 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.57 min0.05 nl5 QC0\n",
      "37\n",
      "T=5 Epoch: 38/40 - Loss: 0.5797 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.53 min0.05 nl5 QC0\n",
      "38\n",
      "T=5 Epoch: 39/40 - Loss: 0.5811 b_loss=0.5730 - T: 0.05s/epoch ,tempo_previto=1.50 min0.05 nl5 QC0\n",
      "39\n",
      "T=5 Epoch: 40/40 - Loss: 0.5668 b_loss=0.5668 - T: 0.05s/epoch ,tempo_previto=1.53 min0.05 nl5 QC0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ancilla_1/all_thetas_T5_nl5_min0.05_qc0_1_ld4.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9e0d746c0564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;31m#print(f'T={T} Epoch: {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.4f} b_loss={best_loss:.4f} - T: {time.time()-t0:.2f}s/epoch ,tempo_previto={(((NUM_EPOCHS-1-epoch+NUM_EPOCHS*(len(qc_array)-q_indx-1)+NUM_EPOCHS*len(qc_array)*(len(min_array)-min_indx-1)+NUM_EPOCHS*len(qc_array)*len(min_array)*(len(layer_array)-layer_indx-1)))):.2f} min{min_b} nl{n_layer}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'ancilla_{Q_ANCILLA}/all_thetas_T{T}_nl{n_layer}_min{min_b}_qc{qc}_{Q_ANCILLA}_ld{ld_dim}.npy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrained_thetas_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'ancilla_{Q_ANCILLA}/all_loss__T{T}_nl{n_layer}_min{min_b}_qc{qc}_ancilla{Q_ANCILLA}_ld{ld_dim}.npy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mfile_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ancilla_1/all_thetas_T5_nl5_min0.05_qc0_1_ld4.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from Modules.training_functions import *\n",
    "from Modules.pennylane_functions import *\n",
    "\n",
    "# if gpu available, set device to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: Could not find GPU, using the CPU\")\n",
    "T=5\n",
    "all=np.load(f'Data/dataset_ld_{ld_dim}_{1}.npy')\n",
    "all=all[:200]\n",
    "for i in range(1):\n",
    "    x=np.load(f'Data/dataset_ld_{ld_dim}_{i}.npy')\n",
    "    all=np.concatenate((x[:200],all))\n",
    "print(np.shape(all))\n",
    "\n",
    "mnist_images=all\n",
    "np.random.shuffle(mnist_images)\n",
    "mnist_images = torch.tensor(mnist_images).to(device)\n",
    "#qc_array=np.array([0,48,60,63]) 4\n",
    "# make dataloader\n",
    "data_loader = torch.utils.data.DataLoader(mnist_images, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "qc_array=np.array([0,32,48,56])\n",
    "min_array=np.array([0.05,0.01,0.005])\n",
    "layer_array=np.array([5,10,20,50]) \n",
    "print(NUM_QUBITS)\n",
    "print(T)\n",
    "zero = torch.zeros(BATCH_SIZE, 2**NUM_QUBITS-ld_dim).to(device)\n",
    "\n",
    "\n",
    "for layer_indx in range(len(layer_array)):\n",
    "    n_layer=layer_array[layer_indx]\n",
    "    for q_indx in range(len(qc_array)):\n",
    "        qc=qc_array[q_indx]\n",
    "        for min_indx in range(len(min_array)):\n",
    "            min_b=min_array[min_indx]\n",
    "\n",
    "            betas      = np.insert(np.linspace(10e-8,min_b, T), 0, 0)\n",
    "            print(np.shape(betas))\n",
    "            alphas     = 1 - betas\n",
    "            alphas_bar = np.cumprod(alphas)\n",
    "            pi         = math.pi\n",
    "            betas      = torch.tensor(betas).float().to(device)\n",
    "            alphas     = torch.tensor(alphas).float().to(device)\n",
    "            alphas_bar = torch.tensor(alphas_bar).float().to(device)\n",
    "            theta_1    = Variable(torch.rand((n_layer*3*NUM_QUBITS+n_layer*3*(NUM_QUBITS)), device = device), requires_grad=True)\n",
    "            optimizer = torch.optim.Adam([theta_1], lr = LEARNING_RATE)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = SCHEDULER_PATIENCE, gamma = SCHEDULER_GAMMA, verbose = False)\n",
    "            trained_thetas_1 = []\n",
    "            loss_history = []\n",
    "            best_loss = 1e10\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print(epoch)\n",
    "\n",
    "                t0 = time.time()\n",
    "                num_batch=0\n",
    "                tot_loss=0\n",
    "\n",
    "                for image_batch in data_loader:\n",
    "\n",
    "                    # extract batch of random times and betas\n",
    "                    t = torch.randint(0, T, size = (BATCH_SIZE, ), device=device)\n",
    "                    betas_batch = betas[t].to(device)\n",
    "                    alphas_batch=alphas_bar[t].to(device)\n",
    "\n",
    "                    # assemble input at t add noise (t+1)\n",
    "                    target_batch = assemble_input(image_batch, t, alphas_bar,ld_dim ,device)\n",
    "                    input_batch  = noise_step(target_batch, t+1, betas,ld_dim, device)\n",
    "                    target_batch = target_batch / torch.norm(target_batch, dim = 1).view(-1, 1)\n",
    "                    input_batch  = input_batch / torch.norm(input_batch, dim = 1).view(-1, 1)\n",
    "                    \n",
    "\n",
    "                    # concatenate the two tensors along the second dimension\n",
    "                    input_batch = torch.cat((input_batch, zero), dim=1)\n",
    "                    target_batch = torch.cat((target_batch, zero), dim=1)\n",
    "                    # Feed to circuit, compute the loss and update the weights\n",
    "                    num_batch+=1\n",
    "                    loss = loss_fn_aq(qc,theta_1,n_layer, input_batch, target_batch)\n",
    "                    tot_loss+=loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # append parameters and print loss\n",
    "                trained_thetas_1.append(theta_1.cpu().clone().detach().numpy())\n",
    "\n",
    "                loss_history.append(tot_loss/num_batch)\n",
    "                if loss.item()< best_loss:\n",
    "                    best_loss=loss.item()\n",
    "\n",
    "                # implement learning rate scheduler\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "            # print every epoch\n",
    "                print(f'T={T} Epoch: {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.4f} b_loss={best_loss:.4f} - T: {time.time()-t0:.2f}s/epoch ,tempo_previto={((time.time()-t0)*(NUM_EPOCHS-1-epoch+NUM_EPOCHS*(len(qc_array)-q_indx-1)+NUM_EPOCHS*len(qc_array)*(len(min_array)-min_indx-1)+NUM_EPOCHS*len(qc_array)*len(min_array)*(len(layer_array)-layer_indx-1)))/60:.2f} min{min_b} nl{n_layer} QC{qc}')\n",
    "                #print(f'T={T} Epoch: {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.4f} b_loss={best_loss:.4f} - T: {time.time()-t0:.2f}s/epoch ,tempo_previto={(((NUM_EPOCHS-1-epoch+NUM_EPOCHS*(len(qc_array)-q_indx-1)+NUM_EPOCHS*len(qc_array)*(len(min_array)-min_indx-1)+NUM_EPOCHS*len(qc_array)*len(min_array)*(len(layer_array)-layer_indx-1)))):.2f} min{min_b} nl{n_layer}')\n",
    "                \n",
    "            np.save(f'ancilla_{Q_ANCILLA}/all_thetas_T{T}_nl{n_layer}_min{min_b}_qc{qc}_{Q_ANCILLA}_ld{ld_dim}.npy',trained_thetas_1)\n",
    "            np.save(f'ancilla_{Q_ANCILLA}/all_loss__T{T}_nl{n_layer}_min{min_b}_qc{qc}_ancilla{Q_ANCILLA}_ld{ld_dim}.npy',loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c015f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf45ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46998c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
